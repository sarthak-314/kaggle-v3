{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nDIR, NAME = user_secrets.get_secret('DIR'), user_secrets.get_secret('NAME')\nimport subprocess\nsubprocess.run(['git', 'clone', NAME, DIR])\n%run $DIR'/covid/startup.py'\n\nFOLD = 0\nBACKBONE, IMG_SIZE = 'efficientnetv2-xl-21k-ft1k', 512\nTFHUB_URL = f'gs://cloud-tpu-checkpoints/efficientnet/v2/hub/{BACKBONE}/feature-vector'\n!pip install -q tensorflow-probability==0.12.1\nsync(); train, valid, test = read_dataframes(FOLD)\nMODEL_CHECKPOINT_DIR = WORKING_DIR / 'Checkpoints'\n\n# Tensorflow Setup\n%run $DIR'/covid/tensorflow/startup.py'\nSTRATEGY = auto_select_accelerator()\n# enable_mixed_precision() ## BUG: Loading from TFHub\n# WEIGHTS_PATH = KAGGLE_INPUT_DIR/'tensorflow-models'/'effnetv2_cxr100.h5'","metadata":{"execution":{"iopub.status.busy":"2021-07-27T11:30:59.430503Z","iopub.execute_input":"2021-07-27T11:30:59.431081Z","iopub.status.idle":"2021-07-27T11:31:48.819977Z","shell.execute_reply.started":"2021-07-27T11:30:59.430956Z","shell.execute_reply":"2021-07-27T11:31:48.818541Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Running on \u001b[32mTPU v3\u001b[0m on Kaggle with internet off\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\nRunning on TPU: grpc://10.0.0.2:8470\nRunning on 8 replicas\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Tensorflow + TPU","metadata":{}},{"cell_type":"code","source":"def build_model(dropout=0.5, num_dense=4): \n    model = tf.keras.Sequential([\n        hub.KerasLayer(TFHUB_URL, trainable=True), \n        tf.keras.layers.Dropout(dropout), \n        tf.keras.layers.Dense(num_dense, kernel_regularizer=tf.keras.regularizers.l2(1e-4)),\n    ])\n    return model\n\n\ndef load_model(weights_path, final_layers, dropout): \n    with STRATEGY.scope(): \n        model = build_model(dropout, final_layers)\n        model.build((None, IMG_SIZE, IMG_SIZE, 3)); model.summary()\n        model.load_weights(weights) \n        model.layers[0].trainable = True\n        for layer in model.layers: layer.trainable = True\n        # model.pop(); model.add(tf.keras.layers.Dense(4))\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-07-27T11:31:48.821981Z","iopub.execute_input":"2021-07-27T11:31:48.822337Z","iopub.status.idle":"2021-07-27T11:31:48.921286Z","shell.execute_reply.started":"2021-07-27T11:31:48.822303Z","shell.execute_reply":"2021-07-27T11:31:48.919796Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Datasets","metadata":{}},{"cell_type":"code","source":"def build_dataset(paths, labels, decode_fn, img_transforms=[], batch_transforms=[], batch_size=4, is_training=False):\n    ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n    ds = ds.map(decode_fn, num_parallel_calls=tf.data.AUTOTUNE)\n    ds = ds.cache().with_options(get_ignore_order())\n    if is_training: ds = ds.repeat().shuffle(512)\n    \n    for img_transform in img_transforms: \n        ds = ds.map(img_transform, num_parallel_calls=tf.data.AUTOTUNE)\n    ds = ds.batch(batch_size, drop_remainder=True)\n    for batch_transform in batch_transforms: \n        ds = ds.map(batch_transform, num_parallel_calls=tf.data.AUTOTUNE)\n    return ds.prefetch(tf.data.AUTOTUNE)\n\ndef get_train_ds(train, batch_size, debug_frac=1):\n    start_time = time()\n    if debug_frac != 1: \n        print(f'Taking {debug_frac*100}% of train ({int(len(train)*debug_frac)} samples), with batch size: {batch_size}')\n        train = train.sample(frac=debug_frac)\n    train_img_transforms = get_img_transforms(IMG_TRANSFORMS, IMG_SIZE, AUG_PARAMS)\n    batch_transforms_fns = get_batch_transforms(BATCH_TRANSFORMS, IMG_SIZE, AUG_PARAMS, NUM_CLASSES, batch_size)\n    img_paths, labels = train.img_path.values, pd.get_dummies(train.label).values\n    train_ds = build_dataset(img_paths, labels, decode_fn, train_img_transforms, batch_transforms_fns, batch_size, True)\n    print(f'{time()-start_time} seconds to load train_ds')\n    train_steps = get_steps(train, batch_size)\n    print(f'{(time()-start_time)*(train_steps/60)} minutes, {train_steps} steps for the first epoch ')\n    return train_ds, train_steps\n\ndef get_clean_ds(df, batch_size, debug_frac=1): \n    start_time = time()\n    if debug_frac != 1: df = df.sample(frac=debug_frac)\n    df_img_transforms = get_img_transforms(['resize'], IMG_SIZE, AUG_PARAMS)\n    batch_transforms = []\n    img_paths, labels = df.img_path.values, pd.get_dummies(df.label).values\n    dataset = build_dataset(img_paths, labels, decode_fn, df_img_transforms, batch_transforms, batch_size, False)\n    return dataset, get_steps(df, batch_size)\n\ndef get_datasets(batch_size, debug_frac): \n    train_ds, train_steps = get_train_ds(train, batch_size, debug_frac)\n    valid_ds, valid_steps = get_clean_ds(valid, batch_size, debug_frac)\n    return train_ds, train_steps, valid_ds, valid_steps","metadata":{"execution":{"iopub.status.busy":"2021-07-27T10:54:11.289065Z","iopub.execute_input":"2021-07-27T10:54:11.289447Z","iopub.status.idle":"2021-07-27T10:54:11.399535Z","shell.execute_reply.started":"2021-07-27T10:54:11.289401Z","shell.execute_reply":"2021-07-27T10:54:11.39847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training Comp Data","metadata":{}},{"cell_type":"code","source":"KAGGLE_DATASET, NUM_CLASSES, IMG_EXT = 'siim-covid19-resized-to-256px-png', 4, 'png'\nDATASET_DIR, decode_fn = KAGGLE_INPUT_DIR/KAGGLE_DATASET, get_decode_fn(IMG_EXT, 3)\n\n# Augmentations Hyperparameters\nIMG_SIZE = 128\nSAMPLING_STRATEGY = None # Oversampling, Undersampling\nOVERSAMPLE_TIMES = {0: 1, 1: 1, 2: 1, 3: 1} \nCLASS_WEIGHTS = { 0: 1, 1: 0.75, 2: 1.25, 3: 2 }\nAUG_PARAMS = {\n    'scale': { 'zoom_in': 0.5, 'zoom_out': 0.9, 'prob': 0.25 }, \n    'rot_prob': 0.5, 'blur': { 'ksize': 5, 'prob': 0.05 }, \n    'gridmask': { 'd1': 5,  'd2': 20,  'rotate': 90,  'ratio': 0.5,  'prob': 0.05 },\n    'cutout': { 'sl': 0.01, 'sh': 0.05,  'rl': 0.5, 'prob': 0.1 }, \n    'cutmix_prob': 0.25, 'mixup_prob': 0.1, \n    'augmix': { 'severity': 1, 'width': 2, 'prob': 0 },  \n}\nIMG_TRANSFORMS = [ 'basic_augmentations', 'random_scale', 'resize', 'random_rotate', 'gridmask', 'random_cutout', 'augmix' ]\nBATCH_TRANSFORMS = ['cutmix', 'mixup']\n\n# Build Training & Validation Dataframes\ntrain, valid, test = read_dataframes(FOLD)\ntrain, valid = add_kaggle_and_gcs_path(train, valid, KAGGLE_DATASET)\ntrain['img_path'], valid['img_path'] = train.gcs_path, valid.gcs_path\n# Oversample or Undersample\n# oversample(train, OVERSAMPLE_TIMES)\n\n# train = oversample(train, OVERSAMPLE_TIMES)\nVISUALIZE, ROWS, COLS = True, 4, 8\nvisualize_augmentations(VISUALIZE, get_train_ds(train, COLS)[0], rows=ROWS, cols=COLS)","metadata":{"execution":{"iopub.status.busy":"2021-07-27T10:54:11.401446Z","iopub.execute_input":"2021-07-27T10:54:11.401909Z","iopub.status.idle":"2021-07-27T10:55:05.643287Z","shell.execute_reply.started":"2021-07-27T10:54:11.401858Z","shell.execute_reply":"2021-07-27T10:55:05.641572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# WEIGHTS_PATH = '/kaggle/working/Checkpoints/freezed_500/weights.h5'\nDROPOUT = 0.9\nwith STRATEGY.scope(): \n    model = build_model(DROPOUT, 4)\n    model.build((None, IMG_SIZE, IMG_SIZE, 3)); model.summary()\n    model.load_weights(WEIGHTS_PATH) \n    # model.pop(); model.add(tf.keras.layers.Dense(4))\n    model.layers[0].trainable = True\n    for layer in model.layers: layer.trainable = True\n\ndef compile_model(model): \n    with STRATEGY.scope():\n        model.compile(\n            loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n            metrics=['accuracy', tf.keras.metrics.AUC(multi_label=True)], \n            optimizer=get_ranger(1e-2), steps_per_execution=32,\n        )\n    print('Model Compiled')","metadata":{"execution":{"iopub.status.busy":"2021-07-27T10:55:05.644878Z","iopub.execute_input":"2021-07-27T10:55:05.645305Z","iopub.status.idle":"2021-07-27T10:56:36.37073Z","shell.execute_reply.started":"2021-07-27T10:55:05.645262Z","shell.execute_reply":"2021-07-27T10:56:36.369535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# (5m + 5m + 0.5s/epoch) Freeze Training (No Augmentations; Imbalanced Class Weights)\nEPOCHS, BATCH_SIZE, DEBUG_FRAC = 500, 4096, 1\nclean_train_ds, train_steps = get_clean_ds(train, BATCH_SIZE, DEBUG_FRAC)\n\nwith STRATEGY.scope(): model.layers[0].trainable = False\nmodel.summary(); compile_model(model)\nmodel.fit(\n    clean_train_ds, steps_per_epoch=train_steps, epochs=EPOCHS, \n    class_weight={0: 1, 1: 0.25, 2: 2, 3: 3}, callbacks=[get_reduce_lr_on_plateau(100)], \n)\nwandb.init('model-saver-effnetv2')\nsave_model(model, MODEL_CHECKPOINT_DIR/'freezed_500')\nwandb.save('/kaggle/working/Checkpoints/freezed_500/weights.h5')","metadata":{"execution":{"iopub.status.busy":"2021-07-27T10:56:36.372379Z","iopub.execute_input":"2021-07-27T10:56:36.372734Z","iopub.status.idle":"2021-07-27T10:56:36.470495Z","shell.execute_reply.started":"2021-07-27T10:56:36.372697Z","shell.execute_reply":"2021-07-27T10:56:36.469177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# (746s + 1s/epoch) Full Training\nEPOCHS, BATCH_SIZE, DEBUG_FRAC = 5000, 256, 1\ntrain_ds, train_steps, valid_ds, valid_steps = get_datasets(BATCH_SIZE, DEBUG_FRAC)\nscheduler_kwargs = {'max_lr': 1e-2, 'min_lr': 1e-5, 'lr_decay': 0.99, 'cycle_length': 25, 'mult_factor': 2 }\n\nwith STRATEGY.scope(): model.trainable=True\nmodel.summary(); compile_model(model)\ncallbacks = [\n    get_model_checkpoint(MODEL_CHECKPOINT_DIR/'comp_full'), get_reduce_lr_on_plateau(100),\n    SGDRScheduler(**scheduler_kwargs, steps_per_epoch=train_steps), \n]\nmodel.fit(\n    train_ds, steps_per_epoch=train_steps, epochs=EPOCHS, callbacks=callbacks, \n    validation_data=valid_ds, validation_steps=valid_steps, # Class Weights\n) \nsave_model(model, MODEL_CHECKPOINT_DIR/'comp_full_3k')","metadata":{"execution":{"iopub.status.busy":"2021-07-27T10:56:36.472616Z","iopub.execute_input":"2021-07-27T10:56:36.473111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}